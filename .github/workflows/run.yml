name: Run Decoder

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:

jobs:
  run-decoder:
    runs-on: ubuntu-latest
    timeout-minutes: 180

    steps:
      # 1ï¸âƒ£ Checkout code
      - name: Checkout repository
        uses: actions/checkout@v3

      # 2ï¸âƒ£ Install gdown and download amazon0302.pkl
      - name: Fetch amazon0302.pkl from Google Drive
        env:
          GDRIVE_FILE_ID: ${{ secrets.GDRIVE_FILE_ID }}
        run: |
          echo "ğŸ“¥ Downloading amazon0302.pkl from Google Drive..."

          FILE_ID="$GDRIVE_FILE_ID"
          FILE_NAME="amazon0302.pkl"

          # Try direct download first
          curl -L -o "$FILE_NAME" "https://drive.google.com/uc?export=download&id=${FILE_ID}" || true

          # If it's still HTML (Google warning page), try token-based download
          if file "$FILE_NAME" | grep -q "HTML"; then
            echo "âš ï¸ Detected HTML page, retrying with confirmation token..."
            CONFIRM=$(curl -sc /tmp/gcookie "https://drive.google.com/uc?export=download&id=${FILE_ID}" | \
                      grep -o 'confirm=[^&]*' | sed 's/confirm=//')
            curl -Lb /tmp/gcookie "https://drive.google.com/uc?export=download&confirm=${CONFIRM}&id=${FILE_ID}" -o "$FILE_NAME"
          fi

          echo "âœ… Download complete. Checking file info..."
          ls -lh "$FILE_NAME"
          file "$FILE_NAME"

      # 3ï¸âƒ£ Verify file content
      - name: Verify file content
        run: |
          echo "ğŸ” Verifying downloaded file..."
          FILE_TYPE=$(file amazon0302.pkl)
          echo "File type: $FILE_TYPE"

          if echo "$FILE_TYPE" | grep -Eq "HTML|ASCII text|Unicode text"; then
            echo "âŒ ERROR: amazon0302.pkl is not a valid binary file (likely an HTML page)."
            exit 1
          fi
          echo "âœ… amazon0302.pkl verified as binary."

      # 4ï¸âƒ£ Pull Docker image
      - name: Pull prebuilt Docker image
        run: |
          echo "ğŸ³ Logging into Docker Hub..."
          docker login -u samribahta -p ${{ secrets.DOCKER_HUB_ACCESS_TOKEN }}
          echo "ğŸ“¦ Pulling latest decoder image..."
          docker pull samribahta/decoder-image:latest

      # 5ï¸âƒ£ Create output directories
      - name: Create output directories
        run: |
          mkdir -p ${{ github.workspace }}/plots/cluster
          mkdir -p ${{ github.workspace }}/results
          mkdir -p ${{ github.workspace }}/ckpt
          chmod -R 777 ${{ github.workspace }}/plots
          chmod -R 777 ${{ github.workspace }}/results
          chmod -R 777 ${{ github.workspace }}/ckpt

      # 6ï¸âƒ£ Check CPU resources
      - name: Check available processors and usage percentage
        id: cpu-check
        run: |
          TOTAL_CPUS=$(nproc)
          USED_CPUS=4
          PERCENT_USED=$(( 100 * USED_CPUS / TOTAL_CPUS ))
          echo "Total available processors: $TOTAL_CPUS"
          echo "Processors used for decoder: $USED_CPUS"
          echo "Percent of processors used: ${PERCENT_USED}%"
          echo "total_cpus=$TOTAL_CPUS" >> $GITHUB_OUTPUT
          echo "percent_used=$PERCENT_USED" >> $GITHUB_OUTPUT

      # 7ï¸âƒ£ Run matcher (train model)
      - name: Run matcher in Docker container
        run: |
          docker run --rm \
            -v ${{ github.workspace }}:/app \
            -e PYTHONUNBUFFERED=1 \
            samribahta/decoder-image:latest \
            bash -c "
              set -e
              echo 'ğŸš€ Starting matcher run on Amazon dataset...'
              python -m subgraph_matching.train \
                --dataset=graph \
                --graph_pkl_path=amazon0302.pkl \
                --node_anchored \
                --batch_size 16 \
                --val_size 128 \
                --model_path=/app/ckpt/model_amazon.pt
              echo 'ğŸ“‚ Checking output directories...'
              ls -la /app/results
              ls -la /app/plots
            "

      # 8ï¸âƒ£ Run decoder
      - name: Run decoder in Docker container
        run: |
          docker run --rm \
            -v ${{ github.workspace }}:/app \
            -e PYTHONUNBUFFERED=1 \
            samribahta/decoder-image:latest \
            bash -c "
              set -e
              echo 'ğŸš€ Starting decoder run on Amazon dataset...'
              python -m subgraph_mining.decoder \
                --dataset=amazon0302.pkl \
                --n_trials=200 \
                --node_anchored \
                --model_path=/app/ckpt/model_amazon.pt \
                --out_path=/app/results/amazon_patterns.pkl \
                --graph_type=directed
              echo 'ğŸ“‚ Checking output directories...'
              ls -la /app/plots/cluster
              ls -la /app/results
            "

      # 9ï¸âƒ£ Final file checks
      - name: Check for generated files
        run: |
          echo "ğŸ“ Checking plots directory:"
          ls -R plots/ || echo "No plots directory found"
          echo "ğŸ“ Checking results directory:"
          ls -R results/ || echo "No results directory found"
          echo "ğŸ“ Checking ckpt directory:"
          ls -R ckpt/ || echo "No ckpt directory found"

      # ğŸ”Ÿ Upload results as artifact
      - name: Upload plots and results as artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: amazon-decoder-results
          path: |
            plots/
            results/
            ckpt/
          retention-days: 30
          if-no-files-found: warn

      # 1ï¸âƒ£1ï¸âƒ£ Upload logs
      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: amazon-decoder-logs
          path: |
            *.log
            *.err
          retention-days: 7
          if-no-files-found: ignore
