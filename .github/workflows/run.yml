name: Run Decoder

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:

jobs:
  run-decoder:
    runs-on: ubuntu-latest
    timeout-minutes: 180

    steps:
      # 1️⃣ Checkout code
      - name: Checkout repository
        uses: actions/checkout@v3

      # 2️⃣ Install gdown and download amazon0302.pkl
      - name: Fetch amazon0302.pkl from Google Drive
        env:
          GDRIVE_FILE_ID: ${{ secrets.GDRIVE_FILE_ID }}
        run: |
          echo "📥 Downloading amazon0302.pkl from Google Drive..."

          FILE_ID="$GDRIVE_FILE_ID"
          FILE_NAME="amazon0302.pkl"

          # Try direct download first
          curl -L -o "$FILE_NAME" "https://drive.google.com/uc?export=download&id=${FILE_ID}" || true

          # If it's still HTML (Google warning page), try token-based download
          if file "$FILE_NAME" | grep -q "HTML"; then
            echo "⚠️ Detected HTML page, retrying with confirmation token..."
            CONFIRM=$(curl -sc /tmp/gcookie "https://drive.google.com/uc?export=download&id=${FILE_ID}" | \
                      grep -o 'confirm=[^&]*' | sed 's/confirm=//')
            curl -Lb /tmp/gcookie "https://drive.google.com/uc?export=download&confirm=${CONFIRM}&id=${FILE_ID}" -o "$FILE_NAME"
          fi

          echo "✅ Download complete. Checking file info..."
          ls -lh "$FILE_NAME"
          file "$FILE_NAME"

      # 3️⃣ Verify file content
      - name: Verify file content
        run: |
          echo "🔍 Verifying downloaded file..."
          FILE_TYPE=$(file amazon0302.pkl)
          echo "File type: $FILE_TYPE"

          if echo "$FILE_TYPE" | grep -Eq "HTML|ASCII text|Unicode text"; then
            echo "❌ ERROR: amazon0302.pkl is not a valid binary file (likely an HTML page)."
            exit 1
          fi
          echo "✅ amazon0302.pkl verified as binary."

      # 4️⃣ Pull Docker image
      - name: Pull prebuilt Docker image
        run: |
          echo "🐳 Logging into Docker Hub..."
          docker login -u samribahta -p ${{ secrets.DOCKER_HUB_ACCESS_TOKEN }}
          echo "📦 Pulling latest decoder image..."
          docker pull samribahta/decoder-image:latest

      # 5️⃣ Create output directories
      - name: Create output directories
        run: |
          mkdir -p ${{ github.workspace }}/plots/cluster
          mkdir -p ${{ github.workspace }}/results
          mkdir -p ${{ github.workspace }}/ckpt
          chmod -R 777 ${{ github.workspace }}/plots
          chmod -R 777 ${{ github.workspace }}/results
          chmod -R 777 ${{ github.workspace }}/ckpt

      # 6️⃣ Check CPU resources
      - name: Check available processors and usage percentage
        id: cpu-check
        run: |
          TOTAL_CPUS=$(nproc)
          USED_CPUS=4
          PERCENT_USED=$(( 100 * USED_CPUS / TOTAL_CPUS ))
          echo "Total available processors: $TOTAL_CPUS"
          echo "Processors used for decoder: $USED_CPUS"
          echo "Percent of processors used: ${PERCENT_USED}%"
          echo "total_cpus=$TOTAL_CPUS" >> $GITHUB_OUTPUT
          echo "percent_used=$PERCENT_USED" >> $GITHUB_OUTPUT

      # 7️⃣ Run matcher (train model)
      - name: Run matcher in Docker container
        run: |
          docker run --rm \
            -v ${{ github.workspace }}:/app \
            -e PYTHONUNBUFFERED=1 \
            samribahta/decoder-image:latest \
            bash -c "
              set -e
              echo '🚀 Starting matcher run on Amazon dataset...'
              python -m subgraph_matching.train \
                --dataset=graph \
                --graph_pkl_path=amazon0302.pkl \
                --node_anchored \
                --batch_size 16 \
                --val_size 128 \
                --model_path=/app/ckpt/model_amazon.pt
              echo '📂 Checking output directories...'
              ls -la /app/results
              ls -la /app/plots
            "

      # 8️⃣ Run decoder
      - name: Run decoder in Docker container
        run: |
          docker run --rm \
            -v ${{ github.workspace }}:/app \
            -e PYTHONUNBUFFERED=1 \
            samribahta/decoder-image:latest \
            bash -c "
              set -e
              echo '🚀 Starting decoder run on Amazon dataset...'
              python -m subgraph_mining.decoder \
                --dataset=amazon0302.pkl \
                --n_trials=200 \
                --node_anchored \
                --model_path=/app/ckpt/model_amazon.pt \
                --out_path=/app/results/amazon_patterns.pkl \
                --graph_type=directed
              echo '📂 Checking output directories...'
              ls -la /app/plots/cluster
              ls -la /app/results
            "

      # 9️⃣ Final file checks
      - name: Check for generated files
        run: |
          echo "📁 Checking plots directory:"
          ls -R plots/ || echo "No plots directory found"
          echo "📁 Checking results directory:"
          ls -R results/ || echo "No results directory found"
          echo "📁 Checking ckpt directory:"
          ls -R ckpt/ || echo "No ckpt directory found"

      # 🔟 Upload results as artifact
      - name: Upload plots and results as artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: amazon-decoder-results
          path: |
            plots/
            results/
            ckpt/
          retention-days: 30
          if-no-files-found: warn

      # 1️⃣1️⃣ Upload logs
      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: amazon-decoder-logs
          path: |
            *.log
            *.err
          retention-days: 7
          if-no-files-found: ignore
