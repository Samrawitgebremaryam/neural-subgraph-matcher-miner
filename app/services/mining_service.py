import os
import json
import uuid
import subprocess
import shutil
from ..config.settings import Config

class MiningService:
    @staticmethod
    def run_miner(input_file_path, job_id=None, **kwargs):
        """
        Runs the subgraph miner on the given input file.
        Returns the parsed JSON results and file paths.
        """
        if job_id is None:
            job_id = str(uuid.uuid4())
        
        shared_job_dir = "/shared/output/{}".format(job_id)
        os.makedirs(shared_job_dir, exist_ok=True)
        
        out_filename = str(uuid.uuid4()) + '.pkl'
        out_path = os.path.join(Config.RESULTS_FOLDER, out_filename)
        json_path = os.path.join(Config.RESULTS_FOLDER, out_filename.replace('.pkl', '.json'))
        
        # Instance files generated by decoder.py
        instance_json_path = os.path.join(Config.RESULTS_FOLDER, out_filename.replace('.pkl', '_all_instances.json'))
        instance_pkl_path = os.path.join(Config.RESULTS_FOLDER, out_filename.replace('.pkl', '_all_instances.pkl'))

        try:
            # Build command dynamically
            cmd = [
                "python3", "-m", "subgraph_mining.decoder",
                "--dataset={}".format(input_file_path),
                "--out_path={}".format(out_path)
            ]
            
            # Map kwargs to command arguments
            if 'n_trials' in kwargs:
                cmd.append("--n_trials={}".format(kwargs['n_trials']))
            
            if 'min_pattern_size' in kwargs:
                cmd.append("--min_pattern_size={}".format(kwargs['min_pattern_size']))
                
            if 'max_pattern_size' in kwargs:
                cmd.append("--max_pattern_size={}".format(kwargs['max_pattern_size']))
            
            if 'graph_type' in kwargs:
                cmd.append("--graph_type={}".format(kwargs['graph_type']))
                
            if kwargs.get('node_anchored', True): # Default to true as it seems common
                cmd.append("--node_anchored")
                
            if kwargs.get('visualize_instances', False):
                cmd.append("--visualize_instances")
            
            print("Running command: {}".format(' '.join(cmd)), flush=True)
            print("Mining started - this may take several minutes...", flush=True)
            print("Job ID: {}".format(job_id), flush=True)
            
            # Use Popen to stream output in real-time
            import os as os_module
            env = os_module.environ.copy()
            env['PYTHONUNBUFFERED'] = '1'
            
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                bufsize=1,
                env=env
            )
            
            # Stream output line by line
            for line in process.stdout:
                print(line.rstrip(), flush=True)
            
            process.wait()
            
            if process.returncode != 0:
                raise Exception("Miner failed with exit code {}".format(process.returncode))

            # Read the results
            if not os.path.exists(json_path):
                 raise Exception('Result file not found')

            with open(json_path, 'r') as f:
                mining_results = json.load(f)

            shared_results_dir = os.path.join(shared_job_dir, "results")
            shared_plots_dir = os.path.join(shared_job_dir, "plots")
            os.makedirs(shared_results_dir, exist_ok=True)
            os.makedirs(shared_plots_dir, exist_ok=True)
            
            # Persistent folders in submodule root (Standard location for LLM and local access)
            persistent_results_dir = Config.RESULTS_FOLDER
            persistent_plots_dir = os.path.join(Config.BASE_DIR, "plots", "cluster")
            os.makedirs(persistent_results_dir, exist_ok=True)
            os.makedirs(persistent_plots_dir, exist_ok=True)

            # 1. Handle Basic Pattern Results
            if os.path.exists(out_path):
                # Copy to shared results for download
                shutil.copy(out_path, os.path.join(shared_results_dir, "patterns.pkl"))
                # Copy to persistent results for latest job context (fixed name)
                shutil.copy(out_path, os.path.join(persistent_results_dir, "patterns.pkl"))
            
            if os.path.exists(json_path):
                # Copy to shared results for download
                shutil.copy(json_path, os.path.join(shared_results_dir, "patterns.json"))
                # Copy to persistent results for latest job context (fixed name)
                shutil.copy(json_path, os.path.join(persistent_results_dir, "patterns.json"))
            
            # 2. Handle Instance Results (CRITICAL for LLM)
            if os.path.exists(instance_json_path):
                # Copy to shared results for download
                shutil.copy(instance_json_path, os.path.join(shared_results_dir, "patterns_all_instances.json"))
                # Copy to persistent results root for LLM context (fixed name)
                shutil.copy(instance_json_path, os.path.join(persistent_results_dir, "patterns_all_instances.json"))
                
            if os.path.exists(instance_pkl_path):
                # Copy to shared results for download
                shutil.copy(instance_pkl_path, os.path.join(shared_results_dir, "patterns_all_instances.pkl"))
                # Copy to persistent results root (fixed name)
                shutil.copy(instance_pkl_path, os.path.join(persistent_results_dir, "patterns_all_instances.pkl"))

            # 3. Handle Plots (Copy to shared volume for download)
            if os.path.exists(persistent_plots_dir):
                print("Syncing plots to shared volume for download...", flush=True)
                for item in os.listdir(persistent_plots_dir):
                    s = os.path.join(persistent_plots_dir, item)
                    d = os.path.join(shared_plots_dir, item)
                    if os.path.isdir(s):
                        if os.path.exists(d):
                            shutil.rmtree(d)
                        shutil.copytree(s, d)
                    else:
                        shutil.copy2(s, d)

            print("✓ Mining results saved to shared volume: {}".format(shared_job_dir), flush=True)
            print("✓ Mining results persisted to submodule results/: {}".format(persistent_results_dir), flush=True)
            print("✓ Mining plots persisted to submodule plots/: {}".format(persistent_plots_dir), flush=True)
            
            print("Results saved to shared volume: {}".format(shared_job_dir), flush=True)
            
            return {
                "motifs": mining_results,
                "job_id": job_id,
                "results_path": "/shared/output/{}/results".format(job_id),
                "plots_path": "/shared/output/{}/plots".format(job_id)
            }

        finally:
            # Cleanup temporary output files
            # Note: We need to clean up instance files too
            if os.path.exists(out_path):
                os.remove(out_path)
            if os.path.exists(json_path):
                os.remove(json_path)
            if os.path.exists(instance_json_path):
                os.remove(instance_json_path)
            if os.path.exists(instance_pkl_path):
                os.remove(instance_pkl_path)
