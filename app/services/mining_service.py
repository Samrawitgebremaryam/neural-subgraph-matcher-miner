import os
import json
import uuid
import subprocess
import shutil
import re
import time
import threading
from ..config.settings import Config

PHASE_WEIGHTS = {"sampling": (0, 20), "search_trials": (20, 95), "saving": (95, 100)}

# Human-readable labels for each phase
PHASE_MESSAGES = {
    "sampling": "Sampling neighborhoods",
    "search_trials": "Search trials",
    "saving": "Saving results & visualizations",
}


class MiningService:
    @staticmethod
    def run_miner(input_file_path, job_id=None, config=None):
        """
        Runs the subgraph miner on the given input file.
        Returns the parsed JSON results and file paths.
        """
        if job_id is None:
            job_id = str(uuid.uuid4())
        
        shared_job_dir = "/shared/output/{}".format(job_id)
        os.makedirs(shared_job_dir, exist_ok=True)
        
        # Clean plots directory to prevent old results from mixing with new ones
        plots_cluster_dir = "/app/plots/cluster"
        if os.path.exists(plots_cluster_dir):
            shutil.rmtree(plots_cluster_dir)
        os.makedirs(plots_cluster_dir, exist_ok=True)
        
        out_filename = str(uuid.uuid4()) + '.pkl'
        out_path = os.path.join(Config.RESULTS_FOLDER, out_filename)
        json_path = os.path.join(Config.RESULTS_FOLDER, out_filename.replace('.pkl', '.json'))
        
        # Instance files generated by decoder.py
        instance_json_path = os.path.join(Config.RESULTS_FOLDER, out_filename.replace('.pkl', '_all_instances.json'))
        instance_pkl_path = os.path.join(Config.RESULTS_FOLDER, out_filename.replace('.pkl', '_all_instances.pkl'))

        try:
            # Build command dynamically
            cmd = [
                "python3", "-m", "subgraph_mining.decoder",
                "--dataset={}".format(input_file_path),
                "--out_path={}".format(out_path)
            ]

            if config:
                if config.get('n_trials'):
                    cmd.append("--n_trials={}".format(config['n_trials']))
                
                if config.get('min_pattern_size'):
                    cmd.append("--min_pattern_size={}".format(config['min_pattern_size']))
                    
                if config.get('max_pattern_size'):
                    cmd.append("--max_pattern_size={}".format(config['max_pattern_size']))

                if config.get('min_neighborhood_size'):
                    cmd.append("--min_neighborhood_size={}".format(config['min_neighborhood_size']))
                    
                if config.get('max_neighborhood_size'):
                    cmd.append("--max_neighborhood_size={}".format(config['max_neighborhood_size']))
                    
                if config.get('n_neighborhoods'):
                    cmd.append("--n_neighborhoods={}".format(config['n_neighborhoods']))
                
                if config.get('graph_type'):
                    cmd.append("--graph_type={}".format(config['graph_type']))
                    
                if config.get('radius'):
                    cmd.append("--radius={}".format(config['radius']))
                    
                if config.get('search_strategy'):
                    cmd.append("--search_strategy={}".format(config['search_strategy']))
                    
                if config.get('sample_method'):
                    cmd.append("--sample_method={}".format(config['sample_method']))
                
                if config.get('out_batch_size') is not None:
                    cmd.append("--out_batch_size={}".format(config['out_batch_size']))
                    
                # Default to true as it seems common
                cmd.append("--node_anchored")
                    
                if config.get('visualize_instances', False):
                    cmd.append("--visualize_instances")
            
            print("Running command: {}".format(' '.join(cmd)), flush=True)
            print("Mining started - this may take several minutes...", flush=True)
            print("Job ID: {}".format(job_id), flush=True)
            
            # Use Popen to stream output in real-time
            import os as os_module
            env = os_module.environ.copy()
            env['PYTHONUNBUFFERED'] = '1'
            
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                bufsize=1,
                env=env
            )
            
            # Stream output line by line
            total_chunks = 1
            current_chunk = 0
            
            progress_file = os.path.join(shared_job_dir, "progress.json")
            # sampling, search_trials, saving (percent, current, total per phase)
            phase_state = {
                "sampling": {"percent": 0, "current": 0, "total": 1},
                "search_trials": {"percent": 0, "current": 0, "total": 1},
                "saving": {"percent": 0, "current": 0, "total": 1},
            }
            #  never decrease (avoids 70% -> 12% when switching phases or out-of-order updates)
            max_overall_seen = [0]
            last_written_overall = [-1]
            last_write_time = [0.0]
            last_message = [None]
            running_flag = [True]
            completed_written = [False]

            def compute_overall(phase, phase_percent):
                lo, hi = PHASE_WEIGHTS.get(phase, (0, 100))
                return min(99, int(lo + (hi - lo) * phase_percent / 100))

            def update_progress(status, progress, message, phase=None, phase_percent=None, phase_current=None, phase_total=None, phases=None):
                if phase and phase in phase_state:
                    phase_state[phase]["percent"] = phase_percent if phase_percent is not None else phase_state[phase]["percent"]
                    if phase_current is not None:
                        phase_state[phase]["current"] = phase_current
                    if phase_total is not None:
                        phase_state[phase]["total"] = phase_total
                progress = min(progress, 100 if status == "completed" else 99)
                payload = {
                    "status": status,
                    "progress": progress,
                    "message": message,
                    "phase": phase or (list(phase_state.keys())[-1] if phase_state else None),
                    "phase_percent": phase_percent,
                    "phases": phases if phases is not None else dict(phase_state),
                }
                with open(progress_file, 'w') as f:
                    json.dump(payload, f, indent=0)
                last_written_overall[0] = progress
                last_write_time[0] = time.time()

            def heartbeat_loop():
                """Re-write progress every 1.5s with 'still running' so UI shows activity when decoder is between updates."""
                while running_flag[0]:
                    time.sleep(1.5)
                    if not running_flag[0]:
                        break
                    if time.time() - last_write_time[0] < 1.0:
                        continue
                    try:
                        progress = last_written_overall[0]
                        msg = (last_message[0] or "Running") + " â€” still running"
                        payload = {
                            "status": "mining",
                            "progress": progress,
                            "message": msg,
                            "phase": list(phase_state.keys())[-1] if phase_state else None,
                            "phase_percent": phase_state.get("search_trials", {}).get("percent", 0) if "search_trials" in phase_state else 0,
                            "phases": dict(phase_state),
                        }
                        with open(progress_file, 'w') as f:
                            json.dump(payload, f, indent=0)
                        last_write_time[0] = time.time()
                    except Exception:
                        pass

            heartbeat = threading.Thread(target=heartbeat_loop, daemon=True)
            heartbeat.start()

            # Only ever increase: ignore out-of-order/stale lines (multiprocessing causes buffered interleaved stdout)
            def maybe_update_from_miner_progress(phase, current, total, percent):
                if completed_written[0]:
                    return
                if phase not in phase_state:
                    phase_state[phase] = {"percent": 0, "current": 0, "total": 1}
                prev_current = phase_state[phase]["current"]
                prev_percent = phase_state[phase]["percent"]
                if current < prev_current or percent < prev_percent:
                    return
                phase_state[phase]["current"] = current
                phase_state[phase]["total"] = total
                phase_state[phase]["percent"] = percent

                now = time.time()
                overall = compute_overall(phase, percent)
                overall = max(max_overall_seen[0], overall)
                max_overall_seen[0] = overall

                # When decoder reports saving at 100%, mark completed immediately so UI shows 100% before process exits
                if phase == "saving" and percent >= 100:
                    completed_written[0] = True
                    update_progress("completed", 100, "Saving results & visualizations (100%)", phase=phase, phase_percent=100, phase_current=current, phase_total=total)
                    return
                # Write when overall increased or at least every 0.12s for real-time feel
                if overall > last_written_overall[0] or (now - last_write_time[0]) >= 0.12:
                    label = PHASE_MESSAGES.get(phase, phase)
                    message = "{} ({}%)".format(label, percent)
                    update_progress(
                        "mining",
                        overall,
                        message,
                        phase=phase,
                        phase_percent=percent,
                        phase_current=current,
                        phase_total=total,
                    )

            # Initialize progress
            update_progress("starting", 0, "Initializing miner...")

            # Regex for [MINER_PROGRESS] phase=search_trials current=1714 total=2000 percent=85
            miner_progress_re = re.compile(
                r"\[MINER_PROGRESS\]\s+phase=(\S+)\s+current=(\d+)\s+total=(\d+)\s+percent=(\d+)"
            )

            for line in process.stdout:
                line_str = line.rstrip()
                print(line_str, flush=True)

                try:
                    # Real-time progress from decoder: [MINER_PROGRESS] phase=X current=Y total=Z percent=W
                    match = miner_progress_re.search(line_str)
                    if match:
                        phase_name, current, total, percent = match.group(1), int(match.group(2)), int(match.group(3)), int(match.group(4))
                        maybe_update_from_miner_progress(phase_name, current, total, percent)
                        continue

                    # Legacy chunk-based progress (if no MINER_PROGRESS in decoder)
                    if "started chunk" in line_str:
                         # "... started chunk 1/4"
                        parts = line_str.split("started chunk")[1].strip().split(" ")[0] # "1/4"
                        current, total = map(int, parts.split("/"))
                        total_chunks = total
                        current_chunk = current
                        
                        # Start of a chunk is roughly (chunk-1)/total
                        base_progress = int(((current_chunk - 1) / total_chunks) * 90)
                        update_progress("mining", base_progress, f"Started processing chunk {current_chunk} of {total_chunks}...")

                    elif "still processing chunk" in line_str:
                        # Bump progress slightly to show activity
                        # "... still processing chunk 1/4"
                        parts = line_str.split("still processing chunk")[1].strip().split(" ")[0]
                        current, total = map(int, parts.split("/"))
                        
                        base_progress = int(((current_chunk - 1) / total_chunks) * 90)
                        active_progress = base_progress + int((1 / total_chunks) * 45) # Halfway through chunk
                        update_progress("mining", active_progress, f"Still processing chunk {current_chunk} of {total_chunks}...")

                    elif "finished chunk" in line_str:
                        # "... finished chunk 1/4"
                        parts = line_str.split("finished chunk")[1].strip().split(" ")[0]
                        current, total = map(int, parts.split("/"))
                        
                        # End of chunk is current/total
                        completed_progress = int((current_chunk / total_chunks) * 90)
                        update_progress("mining", completed_progress, f"Finished chunk {current_chunk} of {total_chunks}")
                        
                except Exception as e:
                    # Don't let parsing errors stop the stream
                    print(f"Warning: Failed to parse progress line: {e}", flush=True)

            running_flag[0] = False
            process.wait()
            
            # Final completion update
            update_progress("completed", 100, "Mining completed successfully!")
            
            if process.returncode != 0:
                raise Exception("Miner failed with exit code {}".format(process.returncode))

            # Read the results
            if not os.path.exists(json_path):
                 raise Exception('Result file not found')

            with open(json_path, 'r') as f:
                mining_results = json.load(f)

            shared_results_dir = os.path.join(shared_job_dir, "results")
            shared_plots_dir = os.path.join(shared_job_dir, "plots")
            os.makedirs(shared_results_dir, exist_ok=True)
            os.makedirs(shared_plots_dir, exist_ok=True)
            
            # Persistent folders in submodule root (Standard location for LLM and local access)
            persistent_results_dir = Config.RESULTS_FOLDER
            persistent_plots_dir = os.path.join(Config.BASE_DIR, "plots", "cluster")
            os.makedirs(persistent_results_dir, exist_ok=True)
            os.makedirs(persistent_plots_dir, exist_ok=True)

            # 1. Handle Basic Pattern Results
            if os.path.exists(out_path):
                # Copy to shared results for download
                shutil.copy(out_path, os.path.join(shared_results_dir, "patterns.pkl"))
                # Copy to persistent results for latest job context (fixed name)
                shutil.copy(out_path, os.path.join(persistent_results_dir, "patterns.pkl"))

            if os.path.exists(json_path):
                # Copy to shared results for download
                shutil.copy(json_path, os.path.join(shared_results_dir, "patterns.json"))

            # 2. Handle Instance Files (when visualize_instances=True)
            if os.path.exists(instance_pkl_path):
                # Copy to shared results for download
                shutil.copy(instance_pkl_path, os.path.join(shared_results_dir, "patterns_all_instances.pkl"))
                print(f"Copied instance PKL file to shared results", flush=True)

            if os.path.exists(instance_json_path):
                # Copy to shared results for download
                shutil.copy(instance_json_path, os.path.join(shared_results_dir, "patterns_all_instances.json"))
                print(f"Copied instance JSON file to shared results", flush=True)

            # 3. Handle Plot Files and Directories
            plots_cluster_dir = "/app/plots/cluster"
            if os.path.exists(plots_cluster_dir):
                # Create cluster subdirectory in shared plots
                shared_cluster_dir = os.path.join(shared_plots_dir, "cluster")
                os.makedirs(shared_cluster_dir, exist_ok=True)

                for filename in os.listdir(plots_cluster_dir):
                    src_path = os.path.join(plots_cluster_dir, filename)
                    dst_path = os.path.join(shared_cluster_dir, filename)

                    if os.path.isfile(src_path):
                        # Copy individual files (representative mode PNG/PDF)
                        shutil.copy(src_path, dst_path)
                    elif os.path.isdir(src_path):
                        # Copy directories (instance mode HTML folders)
                        if os.path.exists(dst_path):
                            shutil.rmtree(dst_path)
                        shutil.copytree(src_path, dst_path)
                        print(f"Copied instance plot directory: {filename}", flush=True)
            
            print("Results saved to shared volume: {}".format(shared_job_dir), flush=True)
            
            return {
                "motifs": mining_results,
                "job_id": job_id,
                "results_path": "/shared/output/{}/results".format(job_id),
                "plots_path": "/shared/output/{}/plots".format(job_id)
            }

        finally:
            # Cleanup temporary output files
            # Note: We need to clean up instance files too
            if os.path.exists(out_path):
                os.remove(out_path)
            if os.path.exists(json_path):
                os.remove(json_path)
            if os.path.exists(instance_json_path):
                os.remove(instance_json_path)
            if os.path.exists(instance_pkl_path):
                os.remove(instance_pkl_path)